{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0VnbuB4mEY5E",
        "outputId": "b7c72660-2369-4a0f-e216-7fe1fa61e5d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learned Q-values:\n",
            "[[ 47.37624218  56.61        39.76268723  46.56115662]\n",
            " [ 54.83962283  62.9         33.15197359  47.27387508]\n",
            " [ 59.31918624  81.          63.62673637  53.54859132]\n",
            " [ 75.18135278  75.53029123  90.          56.7061653 ]\n",
            " [ 49.6571471   -1.9          0.           0.41298285]\n",
            " [ 54.62022395   9.04343537   0.           4.38851122]\n",
            " [  5.39        89.02477331   0.           4.90118849]\n",
            " [ 65.30221279  82.0232944  100.          67.44085168]\n",
            " [  0.           0.           0.           0.        ]\n",
            " [  2.27640755   0.           0.           0.        ]\n",
            " [  6.74155255   0.           0.           0.        ]\n",
            " [  0.           0.           0.           0.        ]]\n",
            "\n",
            "Optimal Policy (0: Up, 1: Right, 2: Down, 3: Left):\n",
            "[[1 1 1 2]\n",
            " [0 0 1 2]\n",
            " [0 0 0 0]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the grid world environment\n",
        "# S: Start, G: Goal, x: Obstacle\n",
        "# Actions: 0 - Up, 1 - Right, 2 - Down, 3 - Left\n",
        "grid_world = np.array([\n",
        "    ['S', ' ', 'x', ' '],\n",
        "    [' ', 'x', ' ', ' '],\n",
        "    [' ', ' ', ' ', 'G']\n",
        "])\n",
        "\n",
        "# Define rewards for each state\n",
        "rewards = {\n",
        "    'S': 0,\n",
        "    ' ': 0,\n",
        "    'x': -10,  # Penalty for hitting an obstacle\n",
        "    'G': 100   # Reward for reaching the goal\n",
        "}\n",
        "\n",
        "# Define parameters\n",
        "gamma = 0.9  # Discount factor\n",
        "alpha = 0.1  # Learning rate\n",
        "epsilon = 0.1  # Epsilon-greedy exploration parameter\n",
        "\n",
        "# Initialize Q-values\n",
        "num_actions = 4\n",
        "num_states = np.prod(grid_world.shape)\n",
        "Q = np.zeros((num_states, num_actions))\n",
        "\n",
        "# Convert grid world to state indices\n",
        "def state_to_index(state):\n",
        "    return np.ravel_multi_index(state, grid_world.shape)\n",
        "\n",
        "# Convert state index to grid world coordinates\n",
        "def index_to_state(index):\n",
        "    return np.unravel_index(index, grid_world.shape)\n",
        "\n",
        "# Perform Q-learning with Temporal Difference\n",
        "num_episodes = 1000\n",
        "for episode in range(num_episodes):\n",
        "    state = (0, 0)  # Starting state\n",
        "    while True:\n",
        "        # Epsilon-greedy action selection\n",
        "        if np.random.rand() < epsilon:\n",
        "            action = np.random.randint(num_actions)\n",
        "        else:\n",
        "            action = np.argmax(Q[state_to_index(state)])\n",
        "\n",
        "        # Take action and observe next state and reward\n",
        "        if action == 0:  # Up\n",
        "            next_state = (max(state[0] - 1, 0), state[1])\n",
        "        elif action == 1:  # Right\n",
        "            next_state = (state[0], min(state[1] + 1, grid_world.shape[1] - 1))\n",
        "        elif action == 2:  # Down\n",
        "            next_state = (min(state[0] + 1, grid_world.shape[0] - 1), state[1])\n",
        "        else:  # Left\n",
        "            next_state = (state[0], max(state[1] - 1, 0))\n",
        "\n",
        "        reward = rewards[grid_world[next_state]]\n",
        "\n",
        "        # Update Q-value using Temporal Difference\n",
        "        next_index = state_to_index(next_state)\n",
        "        Q[state_to_index(state), action] += alpha * (\n",
        "            reward + gamma * np.max(Q[next_index]) - Q[state_to_index(state), action])\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "        if grid_world[state] == 'G':\n",
        "            break\n",
        "\n",
        "# Print the learned Q-values\n",
        "print(\"Learned Q-values:\")\n",
        "print(Q)\n",
        "\n",
        "# Find the optimal policy\n",
        "optimal_policy = np.argmax(Q, axis=1).reshape(grid_world.shape)\n",
        "print(\"\\nOptimal Policy (0: Up, 1: Right, 2: Down, 3: Left):\")\n",
        "print(optimal_policy)\n"
      ]
    }
  ]
}